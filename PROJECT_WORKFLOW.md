# ğŸ”„ Project Workflow - Understanding the PDF Chatbot

This document explains **exactly** how the PDF Chatbot works, step-by-step. Perfect for beginners to LangChain, GenAI, and coding.

---

## ğŸ“š Table of Contents

1. [What is This Project?](#what-is-this-project)
2. [Key Concepts Explained](#key-concepts-explained)
3. [Technology Stack](#technology-stack)
4. [Complete Code Flow](#complete-code-flow)
5. [File-by-File Breakdown](#file-by-file-breakdown)
6. [How LangChain Works Here](#how-langchain-works-here)
7. [API Calls Explained](#api-calls-explained)

---

## What is This Project?

A chatbot that:
- âœ… Reads multiple PDF files
- âœ… Understands what's inside them
- âœ… Answers your questions based on the PDF content
- âœ… Can also answer general questions (like a normal AI)

**Example:**
- You upload company policy PDFs
- You ask: "What is the maternity leave policy?"
- Chatbot searches PDFs and gives you the answer with source

---

## Key Concepts Explained

### 1. **LangChain** ğŸ”—
Think of it as a "toolkit" that helps us build AI applications easily.

**What it does:**
- Connects different AI components together (like LEGO blocks)
- Handles PDF reading, text splitting, searching, and AI responses
- Makes complex AI tasks simple

### 2. **Embeddings** ğŸ§®
Converting text into numbers (vectors) so computers can understand similarity.

**Simple Analogy:**
- "Dog" â†’ [0.2, 0.8, 0.1, ...]
- "Puppy" â†’ [0.21, 0.79, 0.12, ...]
- "Car" â†’ [0.9, 0.1, 0.3, ...]

Notice: "Dog" and "Puppy" have similar numbers (close meaning), but "Car" is different.

**In Our Project:**
- We convert PDF text chunks into embeddings
- When you ask a question, we convert it to embeddings
- Find PDF chunks with similar embeddings
- Those chunks likely contain the answer!

### 3. **Vector Database (FAISS)** ğŸ—„ï¸
A special database that stores embeddings and finds similar ones quickly.

**Simple Analogy:**
Like a library where:
- Books are organized by topic similarity (not alphabetically)
- You describe what you want, and it finds the most relevant books instantly

**In Our Project:**
- Stores all PDF text chunks as embeddings
- When you ask a question, it finds the most relevant chunks in milliseconds

### 4. **Retrieval-Augmented Generation (RAG)** ğŸ¯
The core technique this chatbot uses.

**How it works:**
1. **Retrieve:** Find relevant information from PDFs
2. **Augment:** Add that information to the question
3. **Generate:** Let AI create an answer based on retrieved info

**Without RAG:**
- Question: "What is our maternity policy?"
- AI: "I don't know" (AI doesn't have your company docs)

**With RAG:**
- Question: "What is our maternity policy?"
- System finds relevant PDF section: "Maternity leave is 6 months paid..."
- AI gets question + PDF content
- AI: "According to your policy, maternity leave is 6 months paid..."

### 5. **Chunks** âœ‚ï¸
Breaking large PDFs into smaller pieces.

**Why?**
- AI has a limit on how much text it can read at once
- Smaller chunks = more precise searching
- Better accuracy

**Example:**
```
Big PDF (100 pages) â†’ Split into 150 chunks of ~1000 characters each
```

### 6. **Prompts** ğŸ’¬
Instructions we give to the AI.

**Example Prompt:**
```
You are a helpful assistant. Answer the question based on the following context:

Context: {retrieved_pdf_text}
Question: {user_question}

Answer:
```

---

## Technology Stack

### **1. Streamlit** ğŸ–¥ï¸
**What:** Python library for creating web interfaces
**Role:** Creates the chat interface you see in browser
**File:** `app.py`

### **2. LangChain** ğŸ”—
**What:** Framework for building AI applications
**Role:** Orchestrates everything - PDF processing, searching, AI responses
**File:** `chatbot.py`, `utils.py`

### **3. Google Gemini** ğŸ¤–
**What:** Google's AI model (like ChatGPT)
**Role:** Generates intelligent answers to your questions
**API Call:** Happens inside LangChain

### **4. FAISS** ğŸ—„ï¸
**What:** Vector database by Facebook
**Role:** Stores and searches PDF embeddings super fast
**File:** `chatbot.py` (line 104)

### **5. Sentence Transformers** ğŸ§®
**What:** Creates embeddings from text
**Role:** Converts text to numbers for similarity search
**File:** `chatbot.py` (line 47)

### **6. PyPDF** ğŸ“„
**What:** PDF reading library
**Role:** Extracts text from PDF files
**File:** `utils.py` (line 40)

---

## Complete Code Flow

### ğŸš€ **Step 1: Application Starts** (`app.py`)

```
User runs: streamlit run app.py
â†“
app.py loads
â†“
Creates chatbot instance (calls chatbot.py)
```

**Code:** `app.py` line 17-21
```python
if "chatbot" not in st.session_state:
    st.session_state.chatbot = create_chatbot()
```

---

### ğŸ“‚ **Step 2: Chatbot Initialization** (`chatbot.py`)

#### **2.1: Load Environment Variables**
```
Reads .env file
â†“
Gets API key, model name, settings
```

**Code:** `chatbot.py` line 23
```python
load_dotenv()  # Loads GOOGLE_API_KEY, etc.
```

#### **2.2: Initialize Embeddings Model**
```
Loads sentence-transformers model
â†“
Downloads model (first time only, ~90MB)
â†“
Ready to convert text â†’ embeddings
```

**Code:** `chatbot.py` line 44-51
```python
self.embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

**What happens:** Downloads AI model that converts text to numbers

#### **2.3: Initialize Google Gemini**
```
Connects to Google's AI API
â†“
Uses your API key from .env
â†“
Ready to generate answers
```

**Code:** `chatbot.py` line 54-65
```python
self.llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash",
    google_api_key=api_key,
    temperature=0.7
)
```

**What happens:** Establishes connection to Google's servers

#### **2.4: Auto-Load PDFs**
```
Scans backend/pdfs/ folder
â†“
Finds all .pdf files
â†“
Calls _auto_load_pdfs() method
```

**Code:** `chatbot.py` line 73-120

---

### ğŸ“„ **Step 3: PDF Processing** (`utils.py`)

#### **3.1: Read PDF Files**
```python
For each PDF:
    Open file
    â†“
    Extract text from each page
    â†“
    Combine all pages into one big text
```

**Code:** `utils.py` line 29-54
```python
reader = PdfReader(pdf_path)
for page in reader.pages:
    text = page.extract_text()
```

**Example Output:**
```
"Page 1: Company Policy... Page 2: Leave Policy is..."
```

#### **3.2: Split Text into Chunks**
```python
Take big text
â†“
Split into ~1000 character chunks
â†“
Overlap 200 characters between chunks
```

**Code:** `utils.py` line 107-115
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_text(text)
```

**Why overlap?** So important info at chunk boundaries doesn't get lost.

**Example:**
```
Chunk 1: "...leave policy is 6 months..."
Chunk 2: "...6 months paid maternity leave..."
         â†‘ Overlap ensures context
```

#### **3.3: Create Document Objects**
```python
For each chunk:
    Create Document object
    â†“
    Add metadata (source file name, chunk number)
```

**Code:** `utils.py` line 118-138
```python
doc = Document(
    page_content=chunk,
    metadata={"source": "Maternity_Policy.pdf", "chunk_id": 5}
)
```

---

### ğŸ—„ï¸ **Step 4: Create Vector Database** (`chatbot.py`)

#### **4.1: Convert Chunks to Embeddings**
```python
For each chunk:
    Text â†’ Sentence Transformer â†’ Embedding (768 numbers)
```

**Example:**
```
"Maternity leave is 6 months" â†’ [0.23, 0.67, -0.12, ..., 0.45]
                                   â†‘
                                768 numbers
```

#### **4.2: Store in FAISS**
```python
FAISS database created
â†“
All embeddings stored
â†“
Ready for fast searching
```

**Code:** `chatbot.py` line 104
```python
self.vector_store = FAISS.from_documents(documents, self.embeddings)
```

**What happens:**
- FAISS creates an index (like a table of contents)
- Optimizes for fast similarity search
- All in memory (RAM) for speed

#### **4.3: Create Retriever**
```python
Retriever = search engine for our PDFs
â†“
Configured to return top 4 most relevant chunks
```

**Code:** `chatbot.py` line 107-110
```python
retriever = self.vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)
```

---

### ğŸ”— **Step 5: Create QA Chain** (`chatbot.py`)

#### **What is a Chain?**
A "chain" connects multiple steps together automatically.

**Our QA Chain does:**
```
User Question
    â†“
1. Search PDFs (Retriever)
    â†“
2. Find top 4 relevant chunks
    â†“
3. Combine chunks with question
    â†“
4. Send to Google Gemini
    â†“
5. Get AI answer
    â†“
Return answer + sources
```

**Code:** `chatbot.py` line 112-118
```python
self.qa_chain = ConversationalRetrievalChain.from_llm(
    llm=self.llm,              # Google Gemini
    retriever=retriever,        # PDF search
    memory=self.memory,         # Remember conversation
    return_source_documents=True
)
```

---

### ğŸ’¬ **Step 6: User Asks a Question** (`app.py`)

#### **6.1: User Types in Chat**
```
User types: "What is the maternity policy?"
â†“
Streamlit captures input
â†“
Calls chatbot.ask(question)
```

**Code:** `app.py` line 44-56

#### **6.2: Question Routing** (`chatbot.py`)

**The chatbot is smart! It decides:**

```python
Is it a greeting? (hi, hello)
    â†“ YES â†’ Return friendly greeting
    â†“ NO
    â†“
Does it mention PDF keywords? (company, policy, leave, etc.)
    â†“ YES â†’ Search PDFs
    â†“ NO â†’ Use AI directly for general answer
```

**Code:** `chatbot.py` line 132-190

---

### ğŸ” **Step 7: PDF Search (RAG Process)**

#### **7.1: Question â†’ Embedding**
```python
"What is the maternity policy?"
    â†“
Sentence Transformer
    â†“
[0.15, 0.82, -0.34, ..., 0.61]  # 768 numbers
```

#### **7.2: Similarity Search in FAISS**
```python
FAISS compares question embedding with all chunk embeddings
    â†“
Calculates similarity scores
    â†“
Returns top 4 most similar chunks
```

**Example Results:**
```
Chunk #47 from Maternity_Policy.pdf (similarity: 0.89)
Chunk #48 from Maternity_Policy.pdf (similarity: 0.85)
Chunk #12 from Leave_Policy.pdf (similarity: 0.72)
Chunk #03 from Employee_Benefits.pdf (similarity: 0.68)
```

#### **7.3: Create Context**
```python
Retrieved chunks are combined:

context = """
Chunk 1: Maternity leave policy provides 6 months paid leave...
Chunk 2: ...extended to 8 months in special cases...
Chunk 3: All female employees are eligible...
Chunk 4: Benefits include full salary and health coverage...
"""
```

---

### ğŸ¤– **Step 8: API Call to Google Gemini**

#### **8.1: Build Prompt**
```
LangChain automatically creates a prompt like:

System: You are a helpful assistant. Answer based on the context.

Context:
Maternity leave policy provides 6 months paid leave...
extended to 8 months in special cases...
All female employees are eligible...
Benefits include full salary and health coverage...

Question: What is the maternity policy?

Answer:
```

#### **8.2: Send to Google Gemini API**
```python
Request sent over internet
    â†“
Google's servers receive prompt
    â†“
Gemini AI processes it
    â†“
Generates intelligent answer
    â†“
Response sent back
```

**Code:** Happens inside `chatbot.py` line 155
```python
response = self.qa_chain({"question": question})
```

**Actual API Call (behind the scenes):**
```
POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent
Headers: {
    "Authorization": "Bearer YOUR_API_KEY"
}
Body: {
    "prompt": "System: You are...",
    "temperature": 0.7,
    "max_tokens": 1024
}
```

#### **8.3: Receive Response**
```python
{
    "answer": "According to the policy, maternity leave is 6 months paid,
               extendable to 8 months in special cases. All female employees
               are eligible and receive full salary plus health coverage.",
    "source_documents": [chunk1, chunk2, chunk3, chunk4]
}
```

---

### ğŸ“¤ **Step 9: Format and Display Response** (`app.py`)

#### **9.1: Extract Answer and Sources**
```python
answer = "According to the policy..."
sources = "Maternity_Policy.pdf, Leave_Policy.pdf"
```

**Code:** `chatbot.py` line 156-158
```python
answer = response.get("answer", "")
source_documents = response.get("source_documents", [])
sources = format_sources(source_documents)
```

#### **9.2: Display in Chat**
```python
Streamlit shows:
    Bot: According to the policy, maternity leave is 6 months...

    ğŸ“„ Sources: Maternity_Policy.pdf, Leave_Policy.pdf
```

**Code:** `app.py` line 58-64

---

### ğŸ§  **Step 10: Conversation Memory**

#### **How Memory Works:**
```python
Every question and answer is stored in memory:

Memory:
    User: "What is maternity leave?"
    Bot: "6 months paid..."

    User: "Can it be extended?"  â† Knows "it" = maternity leave
    Bot: "Yes, to 8 months..."
```

**Code:** `chatbot.py` line 37-42
```python
self.memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```

---

## File-by-File Breakdown

### **1. app.py** - User Interface

**Purpose:** Creates the web page you see

**Key Sections:**
```python
Lines 10-14:  Configure page (title, icon)
Lines 17-24:  Initialize chatbot (loads PDFs)
Lines 27-33:  Check if PDFs loaded
Lines 36-41:  Display chat history
Lines 44-79:  Handle user input and show responses
Lines 82-97:  Sidebar (about, clear chat)
```

**Flow:**
```
User opens browser
    â†“
Streamlit loads app.py
    â†“
Creates chatbot
    â†“
Shows chat interface
    â†“
Waits for user input
```

---

### **2. chatbot.py** - Brain of the Application

**Purpose:** All AI logic lives here

**Key Sections:**
```python
Lines 11-17:  Imports (LangChain, AI models, etc.)
Lines 29-70:  __init__ - Initialize everything
Lines 73-120: _auto_load_pdfs - Load and process PDFs
Lines 123-190: ask - Handle user questions
Lines 192-194: is_ready - Check if chatbot ready
Lines 196-198: clear_memory - Reset conversation
```

**Main Classes/Functions:**
- `PDFChatbot` class - Main chatbot
- `create_chatbot()` - Factory function to create chatbot

---

### **3. utils.py** - PDF Processing Utilities

**Purpose:** Read and prepare PDFs

**Key Functions:**
```python
Lines 29-54:  extract_text_from_pdf - Read one PDF
Lines 57-78:  extract_text_from_multiple_pdfs - Read many PDFs
Lines 81-140: chunk_text - Split text into chunks
Lines 143-169: process_pdfs_to_documents - Complete pipeline
Lines 172-198: format_sources - Format source citations
```

---

### **4. .env** - Configuration

**Purpose:** Store secrets and settings

**Contents:**
```env
GOOGLE_API_KEY=your_key_here       # API authentication
GEMINI_MODEL=models/gemini-2.5-flash  # Which AI model
CHUNK_SIZE=1000                     # How big are chunks
CHUNK_OVERLAP=200                   # Overlap between chunks
```

---

### **5. requirements.txt** - Dependencies

**Purpose:** List all required libraries

**Key Libraries:**
```
langchain==0.1.20              # Main framework
langchain-google-genai==1.0.1  # Google AI integration
faiss-cpu>=1.8.0               # Vector database
sentence-transformers>=2.5.1   # Embeddings
pypdf>=4.1.0                   # PDF reading
streamlit>=1.32.0              # Web interface
```

---

## How LangChain Works Here

### **LangChain Components Used:**

#### **1. Document Loaders** âœ…
- **What:** Read data from sources (PDFs)
- **Where:** `utils.py` - `extract_text_from_pdf()`
- **How:** PyPDF extracts text, LangChain wraps it in Document objects

#### **2. Text Splitters** âœ‚ï¸
- **What:** Break large text into chunks
- **Where:** `utils.py` - `RecursiveCharacterTextSplitter`
- **How:** Intelligently splits on paragraphs, sentences, then characters

#### **3. Embeddings** ğŸ§®
- **What:** Convert text to vectors
- **Where:** `chatbot.py` - `HuggingFaceEmbeddings`
- **How:** Uses sentence-transformers model locally

#### **4. Vector Stores** ğŸ—„ï¸
- **What:** Store and search embeddings
- **Where:** `chatbot.py` - `FAISS`
- **How:** Creates searchable index in memory

#### **5. Retrievers** ğŸ”
- **What:** Find relevant documents
- **Where:** `chatbot.py` - `vector_store.as_retriever()`
- **How:** Searches FAISS for similar embeddings

#### **6. Chains** ğŸ”—
- **What:** Connect multiple steps
- **Where:** `chatbot.py` - `ConversationalRetrievalChain`
- **How:** Orchestrates: retrieve â†’ prompt â†’ AI â†’ response

#### **7. Memory** ğŸ§ 
- **What:** Remember conversation history
- **Where:** `chatbot.py` - `ConversationBufferMemory`
- **How:** Stores all previous messages

#### **8. LLMs** ğŸ¤–
- **What:** AI model that generates text
- **Where:** `chatbot.py` - `ChatGoogleGenerativeAI`
- **How:** Sends prompts to Google's API

---

## API Calls Explained

### **API Call #1: Google Gemini (Answer Generation)**

**When:** Every time user asks a PDF question

**Where in Code:** `chatbot.py` line 155
```python
response = self.qa_chain({"question": question})
```

**What Happens Behind the Scenes:**
```
1. LangChain builds prompt with context
2. Sends HTTPS request to Google's API
3. Endpoint: https://generativelanguage.googleapis.com/v1beta/...
4. Headers: Authorization with your API key
5. Body: JSON with prompt, model name, settings
6. Google processes request (takes 1-3 seconds)
7. Returns JSON with answer
8. LangChain parses response
```

**Request Structure:**
```json
{
  "contents": [{
    "parts": [{
      "text": "Context: ....\n\nQuestion: What is maternity policy?\n\nAnswer:"
    }]
  }],
  "generationConfig": {
    "temperature": 0.7,
    "maxOutputTokens": 1024
  }
}
```

**Response Structure:**
```json
{
  "candidates": [{
    "content": {
      "parts": [{
        "text": "According to the policy, maternity leave is..."
      }]
    }
  }]
}
```

### **API Call #2: HuggingFace (Embeddings) - LOCAL**

**When:** During PDF loading and question embedding

**Where in Code:** `chatbot.py` line 47

**Important:** This does NOT call an API! It runs locally on your computer.

**What Happens:**
```
1. Text input
2. Loads model from disk (sentence-transformers)
3. Runs through neural network
4. Outputs 768 numbers
5. All happens on your CPU (no internet needed)
```

---

## Summary: Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER STARTS APPLICATION                                      â”‚
â”‚ Command: streamlit run app.py                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APP.PY - Streamlit Interface                                 â”‚
â”‚ â€¢ Load page configuration                                    â”‚
â”‚ â€¢ Initialize session state                                   â”‚
â”‚ â€¢ Call create_chatbot()                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHATBOT.PY - Initialization                                  â”‚
â”‚ â€¢ Load .env variables                                        â”‚
â”‚ â€¢ Initialize HuggingFace embeddings (local)                  â”‚
â”‚ â€¢ Connect to Google Gemini API                               â”‚
â”‚ â€¢ Create conversation memory                                 â”‚
â”‚ â€¢ Call _auto_load_pdfs()                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UTILS.PY - PDF Processing                                    â”‚
â”‚ â€¢ Find all PDFs in backend/pdfs/                             â”‚
â”‚ â€¢ For each PDF:                                              â”‚
â”‚   - Extract text from all pages                              â”‚
â”‚   - Split into 1000-char chunks (200 overlap)                â”‚
â”‚   - Create Document objects with metadata                    â”‚
â”‚ â€¢ Return all chunks                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHATBOT.PY - Create Vector Store                             â”‚
â”‚ â€¢ Convert all chunks to embeddings (768 numbers each)        â”‚
â”‚ â€¢ Store in FAISS vector database                             â”‚
â”‚ â€¢ Create retriever (search engine)                           â”‚
â”‚ â€¢ Build ConversationalRetrievalChain                         â”‚
â”‚ â€¢ Chatbot ready!                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APP.PY - Display Chat Interface                              â”‚
â”‚ â€¢ Show welcome message                                       â”‚
â”‚ â€¢ Display chat input box                                     â”‚
â”‚ â€¢ Wait for user question...                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER ASKS QUESTION                                           â”‚
â”‚ Example: "What is the maternity policy?"                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHATBOT.PY - Question Routing                                â”‚
â”‚ â€¢ Check if greeting â†’ Return welcome message                 â”‚
â”‚ â€¢ Check if PDF question (keywords) â†’ Search PDFs             â”‚
â”‚ â€¢ Otherwise â†’ Use AI directly                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼ (PDF Question Path)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RETRIEVAL (RAG - Step 1)                                     â”‚
â”‚ â€¢ Convert question to embedding (768 numbers)                â”‚
â”‚ â€¢ Search FAISS for similar chunks                            â”‚
â”‚ â€¢ Get top 4 most relevant chunks                             â”‚
â”‚ â€¢ Calculate similarity scores                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AUGMENTATION (RAG - Step 2)                                  â”‚
â”‚ â€¢ Combine retrieved chunks                                   â”‚
â”‚ â€¢ Build context from chunks                                  â”‚
â”‚ â€¢ Add conversation history                                   â”‚
â”‚ â€¢ Create complete prompt                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GENERATION (RAG - Step 3) - API CALL                         â”‚
â”‚ â€¢ Send prompt to Google Gemini API                           â”‚
â”‚ â€¢ Endpoint: generativelanguage.googleapis.com                â”‚
â”‚ â€¢ Include API key in headers                                 â”‚
â”‚ â€¢ Temperature: 0.7                                           â”‚
â”‚ â€¢ Wait for response (1-3 seconds)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHATBOT.PY - Process Response                                â”‚
â”‚ â€¢ Extract answer text                                        â”‚
â”‚ â€¢ Extract source documents                                   â”‚
â”‚ â€¢ Format sources (file names)                                â”‚
â”‚ â€¢ Save to conversation memory                                â”‚
â”‚ â€¢ Return to app.py                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APP.PY - Display Response                                    â”‚
â”‚ â€¢ Show answer in chat                                        â”‚
â”‚ â€¢ Show sources in expandable section                         â”‚
â”‚ â€¢ Add to chat history                                        â”‚
â”‚ â€¢ Wait for next question...                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Takeaways

### **1. RAG (Retrieval-Augmented Generation) is the Core**
```
Your PDFs â†’ Split â†’ Embed â†’ Store â†’ Search â†’ Add to Prompt â†’ AI Answer
```

### **2. No Training Required**
- We don't train the AI on your PDFs
- We just give AI the relevant chunks when needed
- Fast and efficient!

### **3. Two Types of Processing**
- **PDF Questions:** Search docs + AI
- **General Questions:** Just AI

### **4. One API Call Per Question**
- Only to Google Gemini
- Embeddings run locally (free!)

### **5. Memory Makes it Conversational**
- Remembers previous messages
- Understands context ("it", "that", etc.)

---

**This is exactly how your PDF Chatbot works! ğŸ‰**

Every question goes through this flow, combining retrieval (search) with generation (AI) to give accurate answers based on your documents.
